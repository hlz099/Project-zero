c bcflow.Fsync
c---  lpchs is cumulative value of surface segments with patched b.c.
c---  on current grid level
c---  litmbeg is starting no. in cumulative patched items at "lpchs" patch
c---  lpchcb  is starting (global) location for patched cells on a block
c
      lpchs    = 0
      litmbeg  = 1
      lpchcb   = 1
c
c     nghost is number of ghost cells layers
c     2 on finest grid, 1 on coarse grids
c
      nghost   = 1
      if (isoln.eq.igrid) nghost = 2
c
c-------  begin outer loop on the blocks for interface blocks  -----------
      do 3000 ibloc = 1,nbloc
      ns       =  nseg(ibloc)
c----------  begin outer loop on the segments  ----------------------------
      do 300 iseg = 1,ns
      nbctype  =  imap(1 ,iseg ,ibloc)
      if (nbctype.eq.1 .or. nbctype.eq.0) then
        nface   =  imap(2 ,iseg ,ibloc)
        n1beg   =  imap(3 ,iseg ,ibloc)
        n1end   =  imap(4 ,iseg ,ibloc)
        n2beg   =  imap(5 ,iseg ,ibloc)
        n2end   =  imap(6 ,iseg ,ibloc)
        nblocs  =  imap(7 ,iseg ,ibloc)
        nfaces  =  imap(8 ,iseg ,ibloc)
        n1begs  =  imap(9 ,iseg ,ibloc)
        n1ends  =  imap(10,iseg ,ibloc)
        n2begs  =  imap(11,iseg ,ibloc)
        n2ends  =  imap(12,iseg ,ibloc)
c
        n1cnt   =  iabs (n1ends - n1begs) + 2
        n2cnt   =  iabs (n2ends - n2begs) + 2
        if (nfaces.lt.0) then
c
c         source and target directions do not match
c
          ncnt  =  n1cnt
          n1cnt =  n2cnt
          n2cnt =  ncnt
        endif
        ncnt    =  n1cnt * n2cnt * nghost
c--------------  block-interface/inner-cut  -----------------------------
c       get ghost cell variables from source block
c
# if defined BUILD_MPI
        if (nodes(nblocs)-1.eq.myrank) then
# else
        if (nodes(nblocs).eq.myrank) then
# endif
          ndim    =  imp2(igrid,nblocs) * jmp2(igrid,nblocs) *
     .               kmp2(igrid,nblocs)
c
          do n=0,4
            call bccutget (imn,jmn,kmn,
     .       im  (igrid,nblocs),jm  (igrid,nblocs),km  (igrid,nblocs),
     .       imp2(igrid,nblocs),jmp2(igrid,nblocs),kmp2(igrid,nblocs),
     .         w(m5cc(igrid,nblocs)+n*ndim),
     .       nfaces,n1begs,n1ends,n2begs,n2ends,
     .       nghost,wk2d(1+n*ncnt)                           )
          enddo
c
#if defined(BUILD_PVM) || defined(BUILD_MPI)
c         if target is not local, send ghost cell variables to node
# if defined BUILD_MPI
          if (nodes(ibloc)-1.ne.myrank) then
# else
          if (nodes(ibloc).ne.myrank) then
# endif
            call MPI_Send (wk2d,5*ncnt,RTYPE,
     .                     nodes(ibloc)-1,TAG_FLOW,mycomm,ierr)
          endif
#endif
        endif
c
c       update ghost cell variables on target block
# if defined BUILD_MPI
        if (nodes(ibloc)-1.eq.myrank) then
# else
        if (nodes(ibloc).eq.myrank) then
# endif
          ndim    =  imp2(igrid,ibloc) * jmp2(igrid,ibloc) *
     .               kmp2(igrid,ibloc)
c
#if defined(BUILD_PVM) || defined(BUILD_MPI)
c         receive ghost cell variables from node if not already local
# if defined BUILD_MPI
          if (nodes(nblocs)-1.ne.myrank) then
# else
          if (nodes(nblocs).ne.myrank) then
# endif
            call MPI_Recv (wk2d,5*ncnt,RTYPE,
     .                    nodes(nblocs)-1,TAG_FLOW,mycomm,istat,ierr)
          endif
#endif
c
c         compute pressure at ghost cells
c
          np = 5 *ncnt
          do n=1,ncnt
            np       = np +1
            wk2d(np) = gm1*(wk2d(n+4*ncnt) - .5*(wk2d(n+ncnt)**2 +
     .                 wk2d(n+2*ncnt)**2 +wk2d(n+3*ncnt)**2) / wk2d(n))
            wk2d(np) = max (wk2d(np),0.001)
          enddo
c
          do n=0,4
            call bccutset (imn,jmn,kmn,
     .       im  (igrid,ibloc),jm  (igrid,ibloc),km  (igrid,ibloc),
     .       imp2(igrid,ibloc),jmp2(igrid,ibloc),kmp2(igrid,ibloc),
     .         w(m5cc(igrid,ibloc)+n*ndim),
     .       nface,n1beg,n1end,n2beg,n2end,
     .       nghost,wk2d(1+n*ncnt),n1cnt,n2cnt                )
          enddo
c
          call bccutset (imn,jmn,kmn,
     .     im  (igrid,ibloc),jm  (igrid,ibloc),km  (igrid,ibloc),
     .     imp2(igrid,ibloc),jmp2(igrid,ibloc),kmp2(igrid,ibloc),
     .       p(m1cc(igrid,ibloc)),
     .     nface,n1beg,n1end,n2beg,n2end,
     .     nghost,wk2d(1+5*ncnt),n1cnt,n2cnt                )
        endif
      endif
c
c-----      end loop on segments
  300 continue
 3000 continue
c
c---------- initialize variables on patched boundaries
c
      do 3001 ibloc=1,nbloc
      ns=nseg(ibloc)
      if (ipatchg.eq.0) go to 301
      if (ntpchcb(ibloc,igrid).le.0) go to 301
c
# if defined BUILD_MPI
      if (nodes(ibloc)-1.eq.myrank) then
# else
      if (nodes(ibloc).eq.myrank) then
# endif
c
          if( (m1pch1(ibloc,igrid)+ntpchcb(ibloc,igrid)).gt.mxtpchc)
     .    then
             write (iwrit,'(2x,"dimension conflict for mxtpchc "/)')
             write (iwrit,'(2x,"mxtpchc m1pch1 ntpchc igrid ibloc"/)')
             write (iwrit,'(2x,5i7)') mxtpchc,m1pch1(ibloc,igrid),
     .       ntpchcb(ibloc,igrid),igrid,ibloc
             write (iwrit,'(2x,"stop in bcflow sending ipatchc"/)')
c
             call ERREXIT (nodes)

          endif
        call initpfl (imn,jmn,kmn,
     .    im  (igrid,ibloc), jm  (igrid,ibloc), km  (igrid,ibloc),
     .    imp2(igrid,ibloc), jmp2(igrid,ibloc), kmp2(igrid,ibloc),
     .       w(m5cc(igrid,ibloc)),    p(m1cc(igrid,ibloc)),
     .    eomu(m1cc(igrid,ibloc)),
     .    npchcbf(1,ibloc,igrid),      ipatchc(m1pch1(ibloc,igrid)),
     .    jpatchc(m1pch1(ibloc,igrid)),kpatchc(m1pch1(ibloc,igrid)),
     .    igrid, isoln )
c     else
c       do iface=1,6
c         if (npchcbf(iface,ibloc,igrid).gt.0)
c    .      lpchcb = lpchcb + npchcbf(iface,ibloc,igrid)
c       enddo
      endif
c
c------------- patched interface ---------------------------------------
c
c
      litmbeg  = m1pch2(ibloc,igrid)
      do 320 lpchs=lswpchb(ibloc,igrid)+1,lswpche(ibloc,igrid)
c
c      convert face numbers to tlns3d's convention
c      note: ibloc1 and ibloc are equal (this was already checked)
c
       ibloc1 = lspchb1(lpchs,igrid)
       iface1 = ifacetr(lspchf1 (lpchs,igrid))
       ibloc2 = lspchb2 (lpchs,igrid)
       iface2 = ifacetr(lspchf2 (lpchs,igrid))
       litems = npchitm (lpchs,igrid)
       ncnt   = litems * nghost
c
#if defined(BUILD_PVM) || defined(BUILD_MPI)
       iitmbeg = iitmsa(lpchs,igrid) - litems
#else
       iitmbeg = litmbeg
#endif
c
# if defined BUILD_MPI
       if (nodes(ibloc2)-1.eq.myrank) then
# else
       if (nodes(ibloc2).eq.myrank) then
# endif
         ndim    =  imp2(igrid,ibloc2) * jmp2(igrid,ibloc2) *
     .              kmp2(igrid,ibloc2)
c
         do n=0,4
# if defined BUILD_MPI
           if (nodes(ibloc)-1.eq.myrank) then
# else
           if (nodes(ibloc).eq.myrank) then
# endif
cvn (10-22-97)
           call bcpchget (imn,jmn,kmn,
     .     im  (igrid,ibloc2), jm  (igrid,ibloc2), km  (igrid,ibloc2),
     .     imp2(igrid,ibloc2), jmp2(igrid,ibloc2), kmp2(igrid,ibloc2),
     .     w(m5cc(igrid,ibloc2)+n*ndim),
     .     iface2,ipitmb2(litmbeg),jpitmb2(litmbeg),kpitmb2(litmbeg),
     .     litems,nghost,wk2d(1+n*ncnt))
c
           else
c
           call bcpchget (imn,jmn,kmn,
     .     im  (igrid,ibloc2), jm  (igrid,ibloc2), km  (igrid,ibloc2),
     .     imp2(igrid,ibloc2), jmp2(igrid,ibloc2), kmp2(igrid,ibloc2),
     .     w(m5cc(igrid,ibloc2)+n*ndim),
     .     iface2,ipitmbs(iitmbeg),jpitmbs(iitmbeg),kpitmbs(iitmbeg),
     .     litems,nghost,wk2d(1+n*ncnt))
c
           endif
         enddo
c
#if defined(BUILD_PVM) || defined(BUILD_MPI)
# if defined BUILD_MPI
         if (nodes(ibloc)-1.ne.myrank) then
# else
         if (nodes(ibloc).ne.myrank) then
# endif
           call MPI_Send (wk2d,5*ncnt,RTYPE,
     .                    nodes(ibloc)-1,TAG_FLOW,mycomm,ierr)
         endif
#endif
       endif
c
# if defined BUILD_MPI
       if (nodes(ibloc)-1.eq.myrank) then
# else
       if (nodes(ibloc).eq.myrank) then
# endif
         ndim    =  imp2(igrid,ibloc) * jmp2(igrid,ibloc) *
     .              kmp2(igrid,ibloc)
c
#if defined(BUILD_PVM) || defined(BUILD_MPI)
# if defined BUILD_MPI
         if (nodes(ibloc2)-1.ne.myrank) then
# else
         if (nodes(ibloc2).ne.myrank) then
# endif
           call MPI_Recv (wk2d,5*ncnt,RTYPE,
     .                    nodes(ibloc2)-1,TAG_FLOW,mycomm,istat,ierr)
         endif
#endif
c
         np = 5 *ncnt
         do n=1,ncnt
           np       = np +1
           wk2d(np) = gm1*(wk2d(n+4*ncnt) - .5*(wk2d(n+ncnt)**2 +
     .                wk2d(n+2*ncnt)**2 +wk2d(n+3*ncnt)**2) / wk2d(n))
           wk2d(np) = max (wk2d(np),0.001)
         enddo
c
         do n=0,4
           call bcpchset (imn,jmn,kmn,
     .     im  (igrid,ibloc),jm  (igrid,ibloc),km  (igrid,ibloc),
     .     imp2(igrid,ibloc),jmp2(igrid,ibloc),kmp2(igrid,ibloc),
     .     w(m5cc(igrid,ibloc)+n*ndim),
     .     iface1,ipitmb1(litmbeg),jpitmb1(litmbeg),kpitmb1(litmbeg),
     .     litems,nghost,frc(litmbeg),wk2d(1+n*ncnt))
         enddo
c
         call bcpchset (imn,jmn,kmn,
     .     im  (igrid,ibloc),jm  (igrid,ibloc),km  (igrid,ibloc),
     .     imp2(igrid,ibloc),jmp2(igrid,ibloc),kmp2(igrid,ibloc),
     .     p(m1cc(igrid,ibloc)),
     .     iface1,ipitmb1(litmbeg),jpitmb1(litmbeg),kpitmb1(litmbeg),
     .     litems,nghost,frc(litmbeg),wk2d(1+5*ncnt))
c      endif
c
       litmbeg = litmbeg +litems
       endif
 320  continue
c
 301  continue
c
 3001 continue
      do 3002 ibloc=1,nbloc
c
c---      fill in edges (corners) of block boundaries with extrapolation b.c
c
# if defined BUILD_MPI
      if (nodes(ibloc)-1.eq.myrank) then
# else
      if (nodes(ibloc).eq.myrank) then
# endif
          call bcedgfl (imn,jmn,kmn,
     .    im  (igrid,ibloc),jm  (igrid,ibloc),km  (igrid,ibloc),
     .    imp1(igrid,ibloc),jmp1(igrid,ibloc),kmp1(igrid,ibloc),
     .    imp2(igrid,ibloc),jmp2(igrid,ibloc),kmp2(igrid,ibloc),
     .       w(m5cc(igrid,ibloc)),    p(m1cc(igrid,ibloc)),
     .    eomu(m1cc(igrid,ibloc)),
     .    igrid, isoln              )
c
      endif
c-----      end loop on blocks
 3002 continue
